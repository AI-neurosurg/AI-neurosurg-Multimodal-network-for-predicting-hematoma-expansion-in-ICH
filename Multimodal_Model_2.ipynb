{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9d931",
   "metadata": {
    "id": "98d9d931"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nibabel as nib\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_1rJ5azcY94z",
   "metadata": {
    "id": "_1rJ5azcY94z"
   },
   "source": [
    "# **Preprocessing for clinical variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cp1jXBbUAFi1",
   "metadata": {
    "id": "cp1jXBbUAFi1"
   },
   "source": [
    "## Specify the paths to csv files for clinical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1g9-fNjR3RMy",
   "metadata": {
    "id": "1g9-fNjR3RMy"
   },
   "source": [
    "The csv files should be created separately according to the following classification method.\n",
    "*   training and validation, expansion\n",
    "*   training and validation, no expansion\n",
    "*   test, expansion\n",
    "*   test, no expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aKOpUZq3Bcbd",
   "metadata": {
    "id": "aKOpUZq3Bcbd"
   },
   "source": [
    "Only the following variables should be included in the csv files.\n",
    "*   Anticoagulant use\n",
    "*   Systolic blood pressure, mmHg\n",
    "*   Diastolic blood pressure, mmHg\n",
    "*   PT-INR\n",
    "*   Time from onset to baseline CT scan, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0f379",
   "metadata": {
    "id": "20a0f379"
   },
   "outputs": [],
   "source": [
    "path_tabular_expansion_train = \"PATH\"\n",
    "path_tabular_no_expansion_train = \"PATH\"\n",
    "path_tabular_expansion_test = \"PATH\"\n",
    "path_tabular_no_expansion_test = \"PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JfuX4U7Y7YCp",
   "metadata": {
    "id": "JfuX4U7Y7YCp"
   },
   "source": [
    "## Read clinical variables from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cc60a",
   "metadata": {
    "id": "e09cc60a"
   },
   "outputs": [],
   "source": [
    "df_expansion_train = pd.read_csv(path_tabular_expansion_train)\n",
    "df_no_expansion_train = pd.read_csv(path_tabular_no_expansion_train)\n",
    "df_expansion_test = pd.read_csv(path_tabular_expansion_test)\n",
    "df_no_expansion_test = pd.read_csv(path_tabular_no_expansion_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gDyoeZSA766y",
   "metadata": {
    "id": "gDyoeZSA766y"
   },
   "source": [
    "## Standardize continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GqSA7X7RI7AD",
   "metadata": {
    "id": "GqSA7X7RI7AD"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(pd.concat([df_expansion_train[[\"Systolic blood pressure, mmHg\"]], df_no_expansion_train[[\"Systolic blood pressure, mmHg\"]]]))\n",
    "df_expansion_train[[\"Systolic blood pressure, mmHg\"]] = scaler.transform(df_expansion_train[[\"Systolic blood pressure, mmHg\"]])\n",
    "df_no_expansion_train[[\"Systolic blood pressure, mmHg\"]] = scaler.transform(df_no_expansion_train[[\"Systolic blood pressure, mmHg\"]])\n",
    "df_expansion_test[[\"Systolic blood pressure, mmHg\"]] = scaler.transform(df_expansion_test[[\"Systolic blood pressure, mmHg\"]])\n",
    "df_no_expansion_test[[\"Systolic blood pressure, mmHg\"]] = scaler.transform(df_no_expansion_test[[\"Systolic blood pressure, mmHg\"]])\n",
    "\n",
    "scaler.fit(pd.concat([df_expansion_train[[\"Diastolic blood pressure, mmHg\"]], df_no_expansion_train[[\"Diastolic blood pressure, mmHg\"]]]))\n",
    "df_expansion_train[[\"Diastolic blood pressure, mmHg\"]] = scaler.transform(df_expansion_train[[\"Diastolic blood pressure, mmHg\"]])\n",
    "df_no_expansion_train[[\"Diastolic blood pressure, mmHg\"]] = scaler.transform(df_no_expansion_train[[\"Diastolic blood pressure, mmHg\"]])\n",
    "df_expansion_test[[\"Diastolic blood pressure, mmHg\"]] = scaler.transform(df_expansion_test[[\"Diastolic blood pressure, mmHg\"]])\n",
    "df_no_expansion_test[[\"Diastolic blood pressure, mmHg\"]] = scaler.transform(df_no_expansion_test[[\"Diastolic blood pressure, mmHg\"]])\n",
    "\n",
    "scaler.fit(pd.concat([df_expansion_train[[\"PT-INR\"]], df_no_expansion_train[[\"PT-INR\"]]]))\n",
    "df_expansion_train[[\"PT-INR\"]] = scaler.transform(df_expansion_train[[\"PT-INR\"]])\n",
    "df_no_expansion_train[[\"PT-INR\"]] = scaler.transform(df_no_expansion_train[[\"PT-INR\"]])\n",
    "df_expansion_test[[\"PT-INR\"]] = scaler.transform(df_expansion_test[[\"PT-INR\"]])\n",
    "df_no_expansion_test[[\"PT-INR\"]] = scaler.transform(df_no_expansion_test[[\"PT-INR\"]])\n",
    "\n",
    "scaler.fit(pd.concat([df_expansion_train[[\"Time from onset to baseline CT scan, h\"]], df_no_expansion_train[[\"Time from onset to baseline CT scan, h\"]]]))\n",
    "df_expansion_train[[\"Time from onset to baseline CT scan, h\"]] = scaler.transform(df_expansion_train[[\"Time from onset to baseline CT scan, h\"]])\n",
    "df_no_expansion_train[[\"Time from onset to baseline CT scan, h\"]] = scaler.transform(df_no_expansion_train[[\"Time from onset to baseline CT scan, h\"]])\n",
    "df_expansion_test[[\"Time from onset to baseline CT scan, h\"]] = scaler.transform(df_expansion_test[[\"Time from onset to baseline CT scan, h\"]])\n",
    "df_no_expansion_test[[\"Time from onset to baseline CT scan, h\"]] = scaler.transform(df_no_expansion_test[[\"Time from onset to baseline CT scan, h\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7U-mU5C36c-N",
   "metadata": {
    "id": "7U-mU5C36c-N"
   },
   "source": [
    "## Encode strings as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MIQOEN24KlpE",
   "metadata": {
    "id": "MIQOEN24KlpE"
   },
   "outputs": [],
   "source": [
    "df_expansion_train[\"Anticoagulant use\"] = pd.factorize(df_expansion_train[\"Anticoagulant use\"])[0]\n",
    "df_no_expansion_train[\"Anticoagulant use\"] = pd.factorize(df_no_expansion_train[\"Anticoagulant use\"])[0]\n",
    "df_expansion_test[\"Anticoagulant use\"] = pd.factorize(df_expansion_test[\"Anticoagulant use\"])[0]\n",
    "df_no_expansion_test[\"Anticoagulant use\"] = pd.factorize(df_no_expansion_test[\"Anticoagulant use\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P8152qGX6uAI",
   "metadata": {
    "id": "P8152qGX6uAI"
   },
   "source": [
    "## Convert DataFrame to NumPy array (clinical variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a62ada",
   "metadata": {
    "id": "f6a62ada"
   },
   "outputs": [],
   "source": [
    "tabular_expansion_train_val = df_expansion_train.to_numpy().astype(\"float32\")\n",
    "tabular_no_expansion_train_val = df_no_expansion_train.to_numpy().astype(\"float32\")\n",
    "tabular_expansion_test = df_expansion_test.to_numpy().astype(\"float32\")\n",
    "tabular_no_expansion_test = df_no_expansion_test.to_numpy().astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VwYUuo33edyN",
   "metadata": {
    "id": "VwYUuo33edyN"
   },
   "source": [
    "# **Preprocessing for CT images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d88d0",
   "metadata": {
    "id": "b92d88d0"
   },
   "source": [
    "## Function: Read images and a header (metadata) from a NIfTI file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49863630",
   "metadata": {
    "id": "49863630"
   },
   "outputs": [],
   "source": [
    "def read_image_header(path):\n",
    "    load = nib.load(path)\n",
    "    image = load.get_fdata()\n",
    "    header = load.header\n",
    "    return image, header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f3137",
   "metadata": {
    "id": "4d3f3137"
   },
   "source": [
    "## Functions: Preprocessing for original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8438d6",
   "metadata": {
    "id": "dc8438d6"
   },
   "outputs": [],
   "source": [
    "# scale the CT density (Hounsfield Units) of the images after extracting brain and hematoma\n",
    "def density_scaling(image):\n",
    "    image[image < 0] = 0  # use the area with Hounsfield Units between 0 and 100,\n",
    "    image[image > 100] = 0  # which includes brain and hematoma\n",
    "    image = image / 100\n",
    "    return image\n",
    "\n",
    "# reslice and align the number of slices\n",
    "def reslice_and_align_slice(image, header, new_spacing=2, new_slices=80):\n",
    "    # reslice - set new spacing (new_spacing), with which reslice the images\n",
    "    z_axis_spacing = header['pixdim'][3].round(2)  # get the image spacing from the header\n",
    "    spacing_factor = z_axis_spacing / new_spacing\n",
    "    image = ndimage.zoom(image, (1, 1, spacing_factor))\n",
    "\n",
    "    # align the number of slices - set the new number of slices (new_slices)\n",
    "    slices = header['dim'][3]  # get the number of slices from the header\n",
    "    slices_resliced = slices * spacing_factor  # culculate the number of slices after reslicing\n",
    "    slices_resliced_ = int(slices_resliced.round(0))\n",
    "    if slices_resliced_ > new_slices:  # eliminate redundant slices without brain to match the new number of slices\n",
    "        num_reduce = slices_resliced_ - new_slices\n",
    "        image = np.delete(image, np.s_[:num_reduce], 2)\n",
    "        return image\n",
    "    elif slices_resliced_ < new_slices:  # add slices with values of zero to match the new number of slices\n",
    "        num_add = new_slices - slices_resliced_\n",
    "        ndarray_zero = np.zeros((image.shape[0], image.shape[1], num_add))\n",
    "        image = np.concatenate([ndarray_zero, image], 2)\n",
    "        return image\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "# unify the pixel size\n",
    "def unify(image, header):\n",
    "    # calculate the magnification for resizing the images assuming the new pixel size is 0.5 x 0.5\n",
    "    x_pixel = header['pixdim'][1]  # Since the pixel is square, only the x-axis of the pixel is calculated.\n",
    "    magnification = x_pixel / 0.5\n",
    "    # unify\n",
    "    image = ndimage.zoom(image, (magnification, magnification, 1))\n",
    "    # padding to restore the image shape to '512 x 512 x 80' after the unification\n",
    "    padding_0 = int((513 - image.shape[0]) / 2)\n",
    "    padding_1 = int((513 - image.shape[1]) / 2)\n",
    "    ndarray_zero = np.zeros((padding_0, image.shape[1], image.shape[2]))  # anterior of the brain\n",
    "    image = np.concatenate([ndarray_zero, image], 0)\n",
    "    ndarray_zero = np.zeros((padding_0, image.shape[1], image.shape[2]))  # posterior of the brain\n",
    "    image = np.concatenate([image, ndarray_zero], 0)\n",
    "    ndarray_zero = np.zeros((image.shape[0], padding_1, image.shape[2]))  # right of the brain\n",
    "    image = np.concatenate([ndarray_zero, image], 1)\n",
    "    ndarray_zero = np.zeros((image.shape[0], padding_1, image.shape[2]))  # left of the brain\n",
    "    image = np.concatenate([image, ndarray_zero], 1)\n",
    "    image[image < 0] = 0  # During the unification process, some pixel values may become\n",
    "    image[image > 1] = 1  # slightly lower than 0 or higher than 1, so the excess are aligned to 0 or 1.\n",
    "    return image\n",
    "\n",
    "# resize the images\n",
    "def resize(image,\n",
    "           image_size=256  # the image size to be created\n",
    "           ):\n",
    "    # resize the image size '512 x 512' to the size specified by the parameter\n",
    "    image = ndimage.zoom(image, (image_size/512, image_size/512, 1))\n",
    "    image[image < 0] = 0  # During the resizing process, some pixel values may become\n",
    "    image[image > 1] = 1  # slightly lower than 0 or higher than 1, so the excess are aligned to 0 or 1.\n",
    "    return image\n",
    "\n",
    "# get processed images using above defined functions\n",
    "def get_processed_images(path):\n",
    "    image, header = read_image_header(path)\n",
    "    image = density_scaling(image)\n",
    "    image = reslice_and_align_slice(image, header)\n",
    "    image = unify(image, header)\n",
    "    image = resize(image)\n",
    "    # rotate the images by 90 degrees\n",
    "    image = ndimage.rotate(image, 90, reshape=False)\n",
    "    return image.round(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d46a2",
   "metadata": {
    "id": "509d46a2"
   },
   "source": [
    "## Functions: Preprocessing for masked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd0f99",
   "metadata": {
    "id": "18cd0f99"
   },
   "outputs": [],
   "source": [
    "# reslice and align the number of slices\n",
    "def reslice_and_align_slice_mask(image, header, new_spacing=2, new_slices=80):\n",
    "    # reslice - set new spacing (new_spacing), with which reslice the images\n",
    "    z_axis_spacing = header['pixdim'][3].round(2)  # get the image spacing from the header\n",
    "    spacing_factor = z_axis_spacing / new_spacing\n",
    "    image = ndimage.zoom(image, (1, 1, spacing_factor))\n",
    "    image[image < 0.2] = 0\n",
    "    image[image >= 0.2] = 1\n",
    "\n",
    "    # align the number of slices - set the new number of slices (new_slices)\n",
    "    slices = header['dim'][3]  # get the number of slices from the header\n",
    "    slices_resliced = slices * spacing_factor  # culculate the number of slices after reslicing\n",
    "    slices_resliced_ = int(slices_resliced.round(0))\n",
    "    if slices_resliced_ > new_slices:  # eliminate redundant slices without brain to match the new number of slices\n",
    "        num_reduce = slices_resliced_ - new_slices\n",
    "        image = np.delete(image, np.s_[:num_reduce], 2)\n",
    "        return image\n",
    "    elif slices_resliced_ < new_slices:  # add slices with values of zero to match the new number of slices\n",
    "        num_add = new_slices - slices_resliced_\n",
    "        ndarray_zero = np.zeros((image.shape[0], image.shape[1], num_add))\n",
    "        image = np.concatenate([ndarray_zero, image], 2)\n",
    "        return image\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "# unify the pixel size\n",
    "def unify_mask(image, header):\n",
    "    # calculate the magnification for resizing the images assuming the new pixel size is 0.5 x 0.5\n",
    "    x_pixel = header['pixdim'][1]  # Since the pixel is square, only the x-axis of the pixel is calculated.\n",
    "    magnification = x_pixel / 0.5\n",
    "    # unify\n",
    "    image = ndimage.zoom(image, (magnification, magnification, 1))\n",
    "    # padding to restore the image shape to '512 x 512 x 80' after the unification\n",
    "    padding_0 = int((513 - image.shape[0]) / 2)\n",
    "    padding_1 = int((513 - image.shape[1]) / 2)\n",
    "    ndarray_zero = np.zeros((padding_0, image.shape[1], image.shape[2]))  # anterior of the brain\n",
    "    image = np.concatenate([ndarray_zero, image], 0)\n",
    "    ndarray_zero = np.zeros((padding_0, image.shape[1], image.shape[2]))  # posterior of the brain\n",
    "    image = np.concatenate([image, ndarray_zero], 0)\n",
    "    ndarray_zero = np.zeros((image.shape[0], padding_1, image.shape[2]))  # right of the brain\n",
    "    image = np.concatenate([ndarray_zero, image], 1)\n",
    "    ndarray_zero = np.zeros((image.shape[0], padding_1, image.shape[2]))  # left of the brain\n",
    "    image = np.concatenate([image, ndarray_zero], 1)\n",
    "    image[image < 0.2] = 0   # after the unification, the values of the images are no longer binary,\n",
    "    image[image >= 0.2] = 1  # therefore, binarize with a threshold of 0.2\n",
    "    return image\n",
    "\n",
    "# resize the images\n",
    "def resize_mask(image,\n",
    "                image_size=256  # the image size to be created\n",
    "               ):\n",
    "    # resize the image size '512 x 512' to the size specified by the parameter\n",
    "    image = ndimage.zoom(image, (image_size/512, image_size/512, 1))\n",
    "    image[image < 0.2] = 0   # after resizing, the values of the images are no longer binary,\n",
    "    image[image >= 0.2] = 1  # therefore, binarize with a threshold of 0.2\n",
    "    return image\n",
    "\n",
    "# get processed images using above defined functions\n",
    "def get_processed_images_mask(path):\n",
    "    image, header = read_image_header(path)\n",
    "    image = reslice_and_align_slice_mask(image, header)\n",
    "    image = unify_mask(image, header)\n",
    "    image = resize_mask(image)\n",
    "    # rotate the images by 90 degrees\n",
    "    image = ndimage.rotate(image, 90, reshape=False)\n",
    "    return image.round(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f942a",
   "metadata": {
    "id": "5b0f942a"
   },
   "source": [
    "## Function: Extract hematoma only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65477d86",
   "metadata": {
    "id": "65477d86"
   },
   "outputs": [],
   "source": [
    "def extract_hematoma(path, path_mask):\n",
    "    list = [os.path.join(path, i) for i in sorted(os.listdir(path))]\n",
    "    original = np.array([get_processed_images(j) for j in tqdm(list)])\n",
    "    original = original.astype('float32')\n",
    "    list_mask = [os.path.join(path_mask, i) for i in sorted(os.listdir(path_mask))]\n",
    "    mask = np.array([get_processed_images_mask(j) for j in tqdm(list_mask)])\n",
    "    mask = mask.astype('float32')\n",
    "    hematoma = original - 1 + mask\n",
    "    hematoma[hematoma <= 0] = 0\n",
    "    return hematoma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YA-pOgLW8oBd",
   "metadata": {
    "id": "YA-pOgLW8oBd"
   },
   "source": [
    "## Specify the paths where NIfTI files are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TAOf3RIB0ifH",
   "metadata": {
    "id": "TAOf3RIB0ifH"
   },
   "source": [
    "The NIfTI files should be stored in each directory according to the following classification method.\n",
    "*   training and validation, expansion, original CT images\n",
    "*   training and validation, expansion, masked images\n",
    "*   training and validation, no expansion, original CT images\n",
    "*   training and validation, no expansion, masked images\n",
    "*   test, expansion, original CT images\n",
    "*   test, expansion, masked images\n",
    "*   test, no expansion, original CT images\n",
    "*   test, no expansion, masked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c94ec5",
   "metadata": {
    "id": "b8c94ec5"
   },
   "outputs": [],
   "source": [
    "path_image_expansion_train_val = \"PATH\"\n",
    "path_image_expansion_train_val_mask = \"PATH\"\n",
    "path_image_no_expansion_train_val = \"PATH\"\n",
    "path_image_no_expansion_train_val_mask = \"PATH\"\n",
    "path_image_expansion_test = \"PATH\"\n",
    "path_image_expansion_test_mask = \"PATH\"\n",
    "path_image_no_expansion_test = \"PATH\"\n",
    "path_image_no_expansion_test_mask = \"PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f-WsZNJ48z2k",
   "metadata": {
    "id": "f-WsZNJ48z2k"
   },
   "source": [
    "## Create image data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2be9c",
   "metadata": {
    "id": "2eb2be9c"
   },
   "outputs": [],
   "source": [
    "image_expansion_train_val = extract_hematoma(path_image_expansion_train_val, path_image_expansion_train_val_mask)\n",
    "image_no_expansion_train_val = extract_hematoma(path_image_no_expansion_train_val, path_image_no_expansion_train_val_mask)\n",
    "image_expansion_test = extract_hematoma(path_image_expansion_test, path_image_expansion_test_mask)\n",
    "image_no_expansion_test = extract_hematoma(path_image_no_expansion_test, path_image_no_expansion_test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9JcLzCfrej3s",
   "metadata": {
    "id": "9JcLzCfrej3s"
   },
   "source": [
    "# **Preprocessing for both clinical variables and CT images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0sUzdaoVNcBO",
   "metadata": {
    "id": "0sUzdaoVNcBO"
   },
   "source": [
    "## Store arrays of clinical variables and CT images in the same array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F37BsHLbJck9",
   "metadata": {
    "id": "F37BsHLbJck9"
   },
   "outputs": [],
   "source": [
    "expansion_train_val = [[tabular_expansion_train_val[i], image_expansion_train_val[i]] for i in range(len(tabular_expansion_train_val))]\n",
    "no_expansion_train_val = [[tabular_no_expansion_train_val[i], image_no_expansion_train_val[i]] for i in range(len(tabular_no_expansion_train_val))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RxD7VTLhNhby",
   "metadata": {
    "id": "RxD7VTLhNhby"
   },
   "source": [
    "## Shuffle the data in the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jsl14XW34fjn",
   "metadata": {
    "id": "jsl14XW34fjn"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.shuffle(expansion_train_val)\n",
    "np.random.shuffle(no_expansion_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lnThDtVRM_xc",
   "metadata": {
    "id": "lnThDtVRM_xc"
   },
   "source": [
    "## 70% were assigned to the training set and the rest to the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xLC0T0pnpXSO",
   "metadata": {
    "id": "xLC0T0pnpXSO"
   },
   "outputs": [],
   "source": [
    "expansion_train = expansion_train_val[:int(len(expansion_train_val)*0.7)]\n",
    "expansion_val = expansion_train_val[int(len(expansion_train_val)*0.7):]\n",
    "no_expansion_train = no_expansion_train_val[:int(len(no_expansion_train_val)*0.7)]\n",
    "no_expansion_val = no_expansion_train_val[int(len(no_expansion_train_val)*0.7):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1dVAIK4O1WY",
   "metadata": {
    "id": "m1dVAIK4O1WY"
   },
   "source": [
    "## \tDivide the array into the arrays for clinical variables and CT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba1ee1",
   "metadata": {
    "id": "81ba1ee1"
   },
   "outputs": [],
   "source": [
    "tabular_expansion_train = np.array([expansion_train[i][0] for i in range(len(expansion_train))])\n",
    "tabular_expansion_val = np.array([expansion_val[i][0] for i in range(len(expansion_val))])\n",
    "tabular_no_expansion_train = np.array([no_expansion_train[i][0] for i in range(len(no_expansion_train))])\n",
    "tabular_no_expansion_val = np.array([no_expansion_val[i][0] for i in range(len(no_expansion_val))])\n",
    "image_expansion_train = np.array([expansion_train[i][1] for i in range(len(expansion_train))])\n",
    "image_expansion_val = np.array([expansion_val[i][1] for i in range(len(expansion_val))])\n",
    "image_no_expansion_train = np.array([no_expansion_train[i][1] for i in range(len(no_expansion_train))])\n",
    "image_no_expansion_val = np.array([no_expansion_val[i][1] for i in range(len(no_expansion_val))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V8jVVKAYPTtN",
   "metadata": {
    "id": "V8jVVKAYPTtN"
   },
   "source": [
    "## Data augmentation by rotating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n25dCAH0Gjt_",
   "metadata": {
    "id": "n25dCAH0Gjt_"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy import ndimage\n",
    "angles = [30]\n",
    "for j in range(len(angles)):\n",
    "    for i in tqdm(range(len(image_expansion_train))):\n",
    "        rotation = ndimage.rotate(image_expansion_train[i], angles[j], reshape=False)\n",
    "        rotation[rotation < 0] = 0\n",
    "        rotation[rotation > 1] = 1\n",
    "        rotation = np.expand_dims(rotation, axis=0)\n",
    "        image_expansion_train = np.concatenate((image_expansion_train, rotation), axis=0)\n",
    "        tabular_expansion_train = np.concatenate((tabular_expansion_train, np.expand_dims(tabular_expansion_train[i], axis=0)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JA_2zcAHPnIZ",
   "metadata": {
    "id": "JA_2zcAHPnIZ"
   },
   "source": [
    "## Data augmentation by flipping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BLsNZGBeGtEo",
   "metadata": {
    "id": "BLsNZGBeGtEo"
   },
   "outputs": [],
   "source": [
    "image_expansion_train = np.concatenate((image_expansion_train, np.flip(image_expansion_train, 2)), axis=0)\n",
    "tabular_expansion_train = np.concatenate((tabular_expansion_train, tabular_expansion_train), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TEKJFANsQgS6",
   "metadata": {
    "id": "TEKJFANsQgS6"
   },
   "source": [
    "## Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vq0-feONcTEN",
   "metadata": {
    "id": "vq0-feONcTEN"
   },
   "outputs": [],
   "source": [
    "label_expansion_train = np.array([1 for i in range(len(tabular_expansion_train))])\n",
    "label_expansion_val = np.array([1 for i in range(len(tabular_expansion_val))])\n",
    "label_no_expansion_train = np.array([0 for i in range(len(tabular_no_expansion_train))])\n",
    "label_no_expansion_val = np.array([0 for i in range(len(tabular_no_expansion_val))])\n",
    "label_expansion_test = np.array([1 for i in range(len(tabular_expansion_test))])\n",
    "label_no_expansion_test = np.array([0 for i in range(len(tabular_no_expansion_test))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xMvEhzgJQ2bZ",
   "metadata": {
    "id": "xMvEhzgJQ2bZ"
   },
   "source": [
    "## Concatenate expansion and no expansion arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UJ6hdRbTcTAV",
   "metadata": {
    "id": "UJ6hdRbTcTAV"
   },
   "outputs": [],
   "source": [
    "tabular_x_train = np.concatenate((tabular_expansion_train, tabular_no_expansion_train), axis=0)\n",
    "tabular_x_val = np.concatenate((tabular_expansion_val, tabular_no_expansion_val), axis=0)\n",
    "image_x_train = np.concatenate((image_expansion_train, image_no_expansion_train), axis=0)\n",
    "image_x_val = np.concatenate((image_expansion_val, image_no_expansion_val), axis=0)\n",
    "y_train = np.concatenate((label_expansion_train, label_no_expansion_train), axis=0)\n",
    "y_val = np.concatenate((label_expansion_val, label_no_expansion_val), axis=0)\n",
    "tabular_x_test = np.concatenate((tabular_expansion_test, tabular_no_expansion_test), axis=0)\n",
    "image_x_test = np.concatenate((image_expansion_test, image_no_expansion_test), axis=0)\n",
    "y_test = np.concatenate((label_expansion_test, label_no_expansion_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZUUaIP7HRChf",
   "metadata": {
    "id": "ZUUaIP7HRChf"
   },
   "source": [
    "## Add a dimension to image arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd6294",
   "metadata": {
    "id": "bbfd6294"
   },
   "outputs": [],
   "source": [
    "image_x_train = tf.expand_dims(image_x_train, axis=4)\n",
    "image_x_val = tf.expand_dims(image_x_val, axis=4)\n",
    "image_x_test = tf.expand_dims(image_x_test, axis=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jyNl3a1rZNje",
   "metadata": {
    "id": "jyNl3a1rZNje"
   },
   "source": [
    "#  **Training and validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-hT4OZ20-QNr",
   "metadata": {
    "id": "-hT4OZ20-QNr"
   },
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HbRwEtq5a_02",
   "metadata": {
    "id": "HbRwEtq5a_02"
   },
   "outputs": [],
   "source": [
    "def model_multimodal():\n",
    "\n",
    "  inputs_tabular = keras.Input(len(tabular_expansion_train[1]))\n",
    "  inputs_image = keras.Input((image_expansion_train.shape[1], image_expansion_train.shape[2], image_expansion_train.shape[3], 1))\n",
    "\n",
    "  t = layers.Dense(units=512, kernel_initializer=\"he_normal\")(inputs_tabular)\n",
    "  t = layers.BatchNormalization()(t)\n",
    "  t = layers.Activation(\"relu\")(t)\n",
    "  t = layers.Dropout(0.3)(t)\n",
    "\n",
    "  t = layers.Dense(units=256, kernel_initializer=\"he_normal\")(t)\n",
    "  t = layers.BatchNormalization()(t)\n",
    "  t = layers.Activation(\"relu\")(t)\n",
    "  t = layers.Dropout(0.3)(t)\n",
    "\n",
    "  i = layers.Conv3D(filters=64, kernel_size=(19,19,7), kernel_initializer=\"he_normal\")(inputs_image)\n",
    "  i = layers.BatchNormalization()(i)\n",
    "  i = layers.Activation(\"relu\")(i)\n",
    "  i = layers.MaxPool3D(pool_size=2)(i)\n",
    "\n",
    "  i = layers.Conv3D(filters=64, kernel_size=(19,19,7), kernel_initializer=\"he_normal\")(i)\n",
    "  i = layers.BatchNormalization()(i)\n",
    "  i = layers.Activation(\"relu\")(i)\n",
    "  i = layers.MaxPool3D(pool_size=2)(i)\n",
    "\n",
    "  i = layers.Conv3D(filters=128, kernel_size=(14,14,5), kernel_initializer=\"he_normal\")(i)\n",
    "  i = layers.BatchNormalization()(i)\n",
    "  i = layers.Activation(\"relu\")(i)\n",
    "  i = layers.MaxPool3D(pool_size=2)(i)\n",
    "\n",
    "  i = layers.Conv3D(filters=256, kernel_size=(11,11,4), kernel_initializer=\"he_normal\")(i)\n",
    "  i = layers.BatchNormalization()(i)\n",
    "  i = layers.Activation(\"relu\")(i)\n",
    "  i = layers.MaxPool3D(pool_size=2)(i)\n",
    "\n",
    "  i = layers.GlobalAveragePooling3D()(i)\n",
    "  i = layers.Dense(units=512, activation=\"relu\", kernel_initializer=\"he_normal\")(i)\n",
    "  i = layers.Dropout(0.3)(i)\n",
    "\n",
    "  concat = layers.concatenate([t, i])\n",
    "\n",
    "  ti = layers.Dense(units=256, kernel_initializer=\"he_normal\")(concat)\n",
    "  ti = layers.BatchNormalization()(ti)\n",
    "  ti = layers.Activation(\"relu\")(ti)\n",
    "  ti = layers.Dropout(0.3)(ti)\n",
    "\n",
    "  outputs = layers.Dense(units=1, activation=\"sigmoid\")(ti)\n",
    "\n",
    "  model = keras.Model((inputs_tabular, inputs_image), outputs, name=\"multimodal\")\n",
    "  return model\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model = model_multimodal()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iJX1C9Lw_ATg",
   "metadata": {
    "id": "iJX1C9Lw_ATg"
   },
   "source": [
    "## Compile and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jw6ix0YCa_yd",
   "metadata": {
    "id": "jw6ix0YCa_yd"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"AUC\", tf.keras.metrics.Recall()])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath=os.path.join('PATH', '{epoch:03d}.h5'))\n",
    "model.fit((tabular_x_train, image_x_train), y_train, validation_data=((tabular_x_val, image_x_val), y_val), batch_size=2, epochs=70, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2BrweGsZab2",
   "metadata": {
    "id": "f2BrweGsZab2"
   },
   "source": [
    "# **Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rJSVaopXA9cK",
   "metadata": {
    "id": "rJSVaopXA9cK"
   },
   "source": [
    "## Specify the path to the HFD5 file that had better sensitivity and AUC in the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f574fc",
   "metadata": {
    "id": "a1f574fc"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g9s2lkrdD9Ep",
   "metadata": {
    "id": "g9s2lkrdD9Ep"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L9T8bGSohdDk",
   "metadata": {
    "id": "L9T8bGSohdDk"
   },
   "outputs": [],
   "source": [
    "y_prediction = model.predict((tabular_x_test, image_x_test), batch_size=2, verbose=1)\n",
    "y_prediction[y_prediction < 0.5] = 0\n",
    "y_prediction[y_prediction >= 0.5] = 1\n",
    "y_prediction = y_prediction.astype(\"int64\")\n",
    "result = confusion_matrix(y_test, y_prediction)\n",
    "sensitivity = recall_score(y_test, y_prediction)\n",
    "specificity = recall_score(y_test, y_prediction, pos_label=0)\n",
    "accuracy = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction)\n",
    "print(result)\n",
    "print(\"sensitivity: {:.3f}\".format(sensitivity))\n",
    "print(\"specificity: {:.3f}\".format(specificity))\n",
    "print(\"accuracy: {:.3f}\".format(accuracy))\n",
    "print(\"AUC: {:.3f}\".format(auc))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
